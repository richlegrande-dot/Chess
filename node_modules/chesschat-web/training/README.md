# Chess Coach LLM Training

Training infrastructure for custom chess coaching language model.

## Prerequisites

- Python 3.9 or higher
- 16GB RAM recommended
- 10GB free disk space

## Quick Start

### 1. Set up Python environment

**Windows (PowerShell):**
```powershell
.\scripts\setup_env.ps1
```

**Manual setup:**
```bash
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac
pip install -r requirements.txt
```

### 2. Prepare training data

Export training data from ChessChatWeb:
1. Navigate to "ðŸ“Š Training Data" in the app
2. Click "ðŸ“¥ Export JSONL"
3. Save file to `training/data/raw/training_data.jsonl`

### 3. Process data

```bash
python scripts/process_data.py --input data/raw/training_data.jsonl --output data/processed/
```

This will create:
- `data/processed/train.jsonl` (80% of data)
- `data/processed/val.jsonl` (20% of data)
- `data/processed/stats.json` (dataset statistics)

### 4. Train model

```bash
python scripts/train_model.py --config configs/gpt2_small.yaml
```

Training time: ~30-60 minutes for 500 examples on CPU

### 5. Export to ONNX

```bash
python scripts/export_to_onnx.py --checkpoint checkpoints/best_model.pt --output ../public/models/
```

## Directory Structure

```
training/
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ gpt2_small.yaml        # GPT-2 Small config (recommended)
â”‚   â”œâ”€â”€ gpt2_medium.yaml       # GPT-2 Medium config
â”‚   â””â”€â”€ distilgpt2.yaml        # DistilGPT-2 config (fastest)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_env.ps1          # Environment setup (Windows)
â”‚   â”œâ”€â”€ process_data.py        # Data preprocessing
â”‚   â”œâ”€â”€ train_model.py         # Training script
â”‚   â”œâ”€â”€ export_to_onnx.py      # ONNX export
â”‚   â””â”€â”€ evaluate_quality.py    # Model quality testing
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw JSONL exports
â”‚   â”‚   â””â”€â”€ training_data.jsonl
â”‚   â”œâ”€â”€ processed/             # Processed train/val splits
â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”œâ”€â”€ val.jsonl
â”‚   â”‚   â””â”€â”€ stats.json
â”‚   â””â”€â”€ samples/               # Sample data for testing
â”‚       â””â”€â”€ sample_10.jsonl
â”œâ”€â”€ checkpoints/               # Model checkpoints during training
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â””â”€â”€ checkpoint_epoch_3.pt
â””â”€â”€ models/                    # Final exported models
    â”œâ”€â”€ chess_coach_gpt2.onnx
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ model_config.json
```

## Training Configuration

### GPT-2 Small (Recommended)
- **Parameters:** 124M
- **Model Size:** ~500MB
- **Training Time:** 30-60 minutes (CPU)
- **Browser Inference:** ~100ms
- **Memory:** ~1GB RAM

Best balance of quality and performance.

### GPT-2 Medium
- **Parameters:** 355M
- **Model Size:** ~1.4GB
- **Training Time:** 2-3 hours (CPU)
- **Browser Inference:** ~300ms
- **Memory:** ~2GB RAM

Better quality, slower performance.

### DistilGPT-2
- **Parameters:** 82M
- **Model Size:** ~330MB
- **Training Time:** 20-30 minutes (CPU)
- **Browser Inference:** ~50ms
- **Memory:** ~700MB RAM

Fastest option, lower quality.

## Hyperparameters

Default settings (GPT-2 Small):
```yaml
model: gpt2
batch_size: 4
learning_rate: 5e-5
epochs: 3
max_length: 512
warmup_steps: 100
weight_decay: 0.01
```

Adjust in `configs/gpt2_small.yaml` if needed.

## Training Progress

Training script shows:
- Current epoch
- Training loss
- Validation loss
- Perplexity
- Sample outputs (every 100 steps)
- ETA

Example:
```
Epoch 1/3 [====================] 100%
Train Loss: 2.341  Val Loss: 2.189  Perplexity: 8.93
Sample: "Protect your pieces by moving the knight to safety..."
ETA: 23 minutes
```

## Data Quality Requirements

### Minimum Dataset
- **500 games** for basic model
- **1,000 games** for good quality
- **2,000+ games** for excellent quality

### Quality Checklist
- âœ… Diverse skill levels (beginner â†’ intermediate)
- âœ… Various openings (e4, d4, Nf3, c4, etc.)
- âœ… Different tactical patterns
- âœ… Complete games (not just early resignations)
- âœ… Balanced colors (50% white, 50% black)

Check data quality:
```bash
python scripts/process_data.py --input data/raw/training_data.jsonl --stats-only
```

## Troubleshooting

### "Out of memory" during training
- Reduce `batch_size` in config (try 2 or 1)
- Use DistilGPT-2 instead
- Close other applications

### Training loss not decreasing
- Increase `learning_rate` (try 1e-4)
- Increase `epochs` (try 5-10)
- Check data quality

### Model generates nonsense
- Need more training data (500+ examples)
- Increase `epochs`
- Try different `learning_rate`
- Check that data format is correct

### ONNX export fails
- Update `onnx` package: `pip install -U onnx onnxruntime`
- Check PyTorch version compatibility
- Try exporting with dynamic axes

### Browser model loading fails
- Check model file size (<2GB recommended)
- Verify ONNX file is not corrupted
- Test with smaller model first
- Check browser console for errors

## Performance Benchmarks

Tested on Intel i7-10700K, 32GB RAM:

| Model | Training Time | Inference (CPU) | Inference (Browser) | Quality |
|-------|---------------|----------------|---------------------|---------|
| GPT-2 Small | 45 min | 50ms | 120ms | Good |
| GPT-2 Medium | 2h 15min | 150ms | 400ms | Better |
| DistilGPT-2 | 28 min | 30ms | 80ms | Fair |

*500 training examples, 3 epochs*

## Next Steps

1. **Collect data:** Play games in coaching mode to reach 500+ examples
2. **Train prototype:** Start with 100 examples to test pipeline
3. **Evaluate:** Check model quality with sample games
4. **Iterate:** Retrain with more data and adjusted hyperparameters
5. **Deploy:** Export to ONNX and integrate with web app

## Support

For issues or questions:
1. Check troubleshooting section above
2. Review training logs in `checkpoints/training.log`
3. Test with sample data first
4. Ensure all dependencies installed correctly

## License

This training infrastructure is part of the ChessChat project.
